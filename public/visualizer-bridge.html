<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Polaris Visualizer Bridge</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      html, body {
        width: 100%;
        height: 100%;
        overflow: hidden;
        background: #000;
      }
      /* Canvas iframe - background visualization, no interaction */
      #visualizer-canvas-frame {
        position: absolute;
        inset: 0;
        width: 100%;
        height: 100%;
        border: none;
        z-index: 1;
        pointer-events: none;
      }
      /* GUI iframe - controls overlay, high z-index for interaction */
      #visualizer-gui-frame {
        position: absolute;
        inset: 0;
        width: 100%;
        height: 100%;
        border: none;
        z-index: 1001;
        background: transparent;
        pointer-events: auto;
      }
      #loading {
        position: absolute;
        inset: 0;
        display: flex;
        align-items: center;
        justify-content: center;
        color: #fff;
        font-family: system-ui, -apple-system, sans-serif;
        font-size: 14px;
        pointer-events: none;
        z-index: 2000;
      }
      #loading.hidden {
        display: none;
      }
    </style>
  </head>
  <body>
    <div id="loading">Loading visualizer...</div>
    <!-- Canvas iframe: renders visualization only -->
    <iframe id="visualizer-canvas-frame" allow="autoplay"></iframe>
    <!-- GUI iframe: shows controls only, transparent background -->
    <iframe id="visualizer-gui-frame" allow="autoplay"></iframe>

    <script type="module">
      // This bridge loads the visualizer in TWO nested iframes (dual-iframe architecture)
      // and proxies messages between the Polaris player (parent) and both visualizer iframes.
      // Canvas iframe: renders visualization only (?mode=canvas)
      // GUI iframe: shows controls only (?mode=gui)
      
      const canvasFrame = document.getElementById('visualizer-canvas-frame');
      const guiFrame = document.getElementById('visualizer-gui-frame');
      const loading = document.getElementById('loading');
      
      // Prevent iframes from stealing focus
      [canvasFrame, guiFrame].forEach(frame => {
        if (frame) {
          frame.tabIndex = -1;
          frame.addEventListener('load', () => {
            try { frame.blur(); } catch { /* ignore */ }
            try { window.focus(); } catch { /* ignore */ }
          });
        }
      });
      
      // Path to the actual visualizer
      // Canvas mode: visualization only, no GUI controls
      // GUI mode: controls only, transparent background
      const VISUALIZER_BASE_PATH = '../visualizer/index.html';
      const CANVAS_PATH = `${VISUALIZER_BASE_PATH}?mode=canvas&autostart=1`;
      const GUI_PATH = `${VISUALIZER_BASE_PATH}?mode=gui&autostart=1`;
      
      const LOG_VERBOSE = false;
      const log = (...args) => LOG_VERBOSE && console.log(...args);

      let visualizerReady = false;
      let externalAudioFeed = false;
      let visualizerInitialized = false;
      let audioElement = null;
      let audioContext = null;
      let analyserNode = null;
      let gainNode = null;
      let sourceNode = null;
      let audioDataRafId = null;
      let vizPlayback = null;
      let vizLoadToken = 0;
      let moduleListReceived = false;
      let modulePollTimer = null;
      let pendingAutoplay = false;

      // Send message to both iframes
      function postToVisualizers(message) {
        try {
          if (canvasFrame?.contentWindow) {
            canvasFrame.contentWindow.postMessage(message, '*');
          }
          if (guiFrame?.contentWindow) {
            guiFrame.contentWindow.postMessage(message, '*');
          }
        } catch (err) {
          if (LOG_VERBOSE) console.warn('[Bridge] Failed to post to visualizers:', err);
        }
      }

      function requestModuleList() {
        try {
          postToVisualizers({ type: 'LIST_MODULES' });
          if (Math.random() < 0.2) log('[Bridge] ‚Üí LIST_MODULES sent to both iframes');
        } catch (err) {
          if (Math.random() < 0.1) console.warn('[Bridge] Module list request failed:', err);
        }
      }

      // Send initialization commands to both visualizer iframes via postMessage
      function initializeVisualizer() {
        if (visualizerInitialized) return;
        
        try {
          // Send to both canvas and GUI iframes
          postToVisualizers({
            type: 'BRIDGE_INIT',
            hideUI: true,
            autoStart: true
          });
          
          visualizerInitialized = true;
          log('[Bridge] Sent initialization to both visualizer iframes');
          // Also prime module list right away
          requestModuleList();
        } catch (err) {
          console.warn('[Bridge] Could not send init message:', err);
        }
      }

      // Parse .viz precomputed visualization files
      function parseVizBuffer(buffer) {
        const view = new DataView(buffer);
        if (view.byteLength < 28) throw new Error('Visualization file too small');

        const magic = String.fromCharCode(...new Uint8Array(buffer.slice(0, 4)));
        if (magic !== 'VIZ1') throw new Error('Invalid visualization header');

        const fps = view.getUint8(5) || 30;
        const bins = view.getUint16(6, true);
        const frames = view.getUint32(8, true);
        const duration = view.getFloat32(12, true) || (frames / Math.max(1, fps));
        const bpm = view.getUint16(16, true);

        const dataOffset = 28;
        const expectedLength = frames * bins;
        if (view.byteLength < dataOffset + expectedLength) {
          throw new Error('Visualization data incomplete');
        }

        const data = new Uint8Array(buffer, dataOffset, expectedLength);
        return { fps, bins, frames, duration, bpm, data };
      }

      function stopVizPlayback({ ended = false, silent = false } = {}) {
        if (vizPlayback?.timerId) {
          clearInterval(vizPlayback.timerId);
        }

        postToVisualizers({
          type: 'PLAYBACK_STATE',
          playing: false
        });

        if (!silent) {
          notifyParent({ type: ended ? 'ENDED' : 'PAUSED' });
        }

        vizPlayback = null;
      }

      function startVizPlayback(vizData) {
        if (!vizData) return;

        stopVizPlayback({ silent: true });

        vizPlayback = {
          ...vizData,
          timerId: null,
          frameIndex: 0,
          playing: false
        };

        if (vizData.bpm) {
          postToVisualizers({ type: 'BPM_DATA', bpm: vizData.bpm });
        }

        notifyParent({ type: 'READY' });
        postToVisualizers({
          type: 'PLAYBACK_STATE',
          playing: false
        });

        const intervalMs = 1000 / Math.max(1, vizData.fps || 30);

        vizPlayback.timerId = setInterval(() => {
          if (!vizPlayback || !vizPlayback.playing) return;

          if (vizPlayback.frameIndex >= vizPlayback.frames) {
            stopVizPlayback({ ended: true });
            return;
          }

          const start = vizPlayback.frameIndex * vizPlayback.bins;
          const frameData = vizPlayback.data.subarray(start, start + vizPlayback.bins);

          // Expand to 2048 bins so the visualizer sees the same shape as live audio analyser
          const TARGET_LEN = 2048;
          let frequencyData;
          if (vizPlayback.bins === TARGET_LEN) {
            frequencyData = frameData;
          } else {
            const expanded = new Uint8Array(TARGET_LEN);
            for (let i = 0; i < TARGET_LEN; i++) {
              const srcIdx = Math.min(vizPlayback.bins - 1, Math.floor(i * vizPlayback.bins / TARGET_LEN));
              expanded[i] = frameData[srcIdx];
            }
            frequencyData = expanded;
          }

          try {
            postToVisualizers({
              type: 'AUDIO_DATA',
              frequencyData: Array.from(frequencyData),
              bufferLength: TARGET_LEN
            });
          } catch (err) {
            // Swallow occasional postMessage errors
          }

          vizPlayback.frameIndex += 1;
        }, intervalMs);
      }

      async function loadVizFromUrl(vizUrl) {
        const token = ++vizLoadToken;
        stopVizPlayback({ silent: true });

        notifyParent({ type: 'LOADING' });

        try {
          const res = await fetch(vizUrl, { cache: 'force-cache' });
          if (!res.ok) throw new Error(`Failed to fetch viz (${res.status})`);

          const buffer = await res.arrayBuffer();
          if (token !== vizLoadToken) return; // superseded

          const vizData = parseVizBuffer(buffer);
          startVizPlayback(vizData);
        } catch (err) {
          if (token !== vizLoadToken) return;
          console.warn('[Bridge] Visualization load failed:', err);
          notifyParent({ type: 'ERROR', error: err?.message || 'Visualization load failed' });
        }
      }

      function messageNeedsAudio(msg) {
        if (!msg || typeof msg !== 'object') return false;
        if (msg.type === 'LOAD_TRACK') return !!msg.url;
        const audioCommands = ['PLAY', 'PAUSE', 'STOP', 'SEEK', 'SET_VOLUME', 'SET_MUTED', 'SET_RATE'];
        return audioCommands.includes(msg.type) && !!audioElement;
      }

      async function attemptAutoplay() {
        if (!pendingAutoplay || !audioElement) return;
        try {
          if (audioContext && audioContext.state === 'suspended') {
            await audioContext.resume();
          }
          await audioElement.play();
          pendingAutoplay = false;
          notifyParent({ type: 'PLAYING' });
        } catch (err) {
          console.warn('[Bridge] Autoplay failed, will retry on next user gesture or PLAY:', err?.message || err);
        }
      }

      // Create audio element for playback
      function initAudio() {
        if (audioElement) {
          log('[Bridge] Audio element already initialized');
          return;
        }
        
        log('[Bridge] Creating audio element...');
        audioElement = document.createElement('audio');
        audioElement.crossOrigin = 'anonymous';
        audioElement.loop = false;
        audioElement.volume = 1.0;
        log('[Bridge] Audio element created:', audioElement);
        
        // Set up Web Audio API for visualization
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        log('[Bridge] AudioContext created, state:', audioContext.state);
        
        sourceNode = audioContext.createMediaElementSource(audioElement);
        analyserNode = audioContext.createAnalyser();
        analyserNode.fftSize = 2048;
        // Disable analyser smoothing so peaks decay immediately (visualizer handles its own smoothing).
        analyserNode.smoothingTimeConstant = 0.0;

        // Mute via gain so analyser still receives live data when "No audio" is enabled.
        gainNode = audioContext.createGain();
        gainNode.gain.value = 1;

        // Connect: source -> analyser -> gain -> destination
        sourceNode.connect(analyserNode);
        analyserNode.connect(gainNode);
        gainNode.connect(audioContext.destination);
        log('[Bridge] Audio graph connected: source -> analyser -> destination');
        
        // Forward audio events to parent
        audioElement.addEventListener('ended', () => {
          stopAudioDataStream();
          notifyParent({ type: 'ENDED' });
        });
        
        audioElement.addEventListener('error', (e) => {
          notifyParent({ type: 'ERROR', error: 'Audio playback error' });
        });
        
        audioElement.addEventListener('play', () => {
          notifyParent({ type: 'PLAYING' });
          
          // Send playing state to both visualizer iframes
          postToVisualizers({
            type: 'PLAYBACK_STATE',
            playing: true
          });
          
          startAudioDataStream(); // Start sending frequency data
          detectAndSendBPM(); // Detect BPM
        });
        
        audioElement.addEventListener('pause', () => {
          if (!audioElement.ended) {
            notifyParent({ type: 'PAUSED' });
          }

          stopAudioDataStream();
          
          // Send paused state to both visualizer iframes
          postToVisualizers({
            type: 'PLAYBACK_STATE',
            playing: false
          });
        });
        
        audioElement.addEventListener('waiting', () => {
          notifyParent({ type: 'BUFFERING' });
        });
        
        audioElement.addEventListener('canplay', () => {
          notifyParent({ type: 'READY' });
        });
        
        audioElement.addEventListener('loadedmetadata', () => {
          notifyParent({ type: 'READY' });
        });
      }

      // Send messages to parent (Polaris Player)
      function notifyParent(message) {
        if (window.parent && window.parent !== window) {
          window.parent.postMessage(message, '*');
        }
      }

      function handleVisualizerMessage(msg) {
        if (!msg || typeof msg !== 'object') return;
        switch (msg.type) {
          case 'MODULE_LIST':
            moduleListReceived = true;
            log('[Bridge] ‚Üê MODULE_LIST', msg);
            notifyParent({
              type: 'VISUALIZER_MODULES',
              modules: Array.isArray(msg.modules) ? msg.modules : [],
              active: msg.active || null
            });
            break;
          case 'MODULE_SET':
            moduleListReceived = true;
            log('[Bridge] ‚Üê MODULE_SET', msg);
            notifyParent({
              type: 'VISUALIZER_MODULE_SET',
              ok: msg.ok === true,
              active: msg.active || null,
              modules: Array.isArray(msg.modules) ? msg.modules : []
            });
            break;
        }
      }

      // Handle commands from parent (Polaris Player)
      window.addEventListener('message', async (event) => {
        const msg = event.data;
        if (!msg || typeof msg !== 'object') return;

        // Check if message is from either visualizer iframe
        const fromCanvasFrame = event.source === canvasFrame?.contentWindow;
        const fromGuiFrame = event.source === guiFrame?.contentWindow;
        const fromVisualizer = fromCanvasFrame || fromGuiFrame;
        
        if (fromVisualizer) {
          handleVisualizerMessage(msg);
          return;
        }
        
        log('[Bridge] Received command:', msg.type, msg);

        // Initialize audio on first command
        if (!audioElement && messageNeedsAudio(msg)) {
          log('[Bridge] Initializing audio element');
          initAudio();
        }

        switch (msg.type) {
          case 'LOAD_TRACK':
            pendingAutoplay = !!msg.autoplay;
            // Direct URL loads imply the bridge will handle playback/analyser.
            if (msg.url) {
              externalAudioFeed = false;
            }
            if (msg.vizUrl) {
              loadVizFromUrl(msg.vizUrl);
            } else if (vizPlayback) {
              stopVizPlayback({ silent: true });
            }

            if (msg.url && audioElement) {
              log('[Bridge] Loading track:', msg.url);
              // Stop current playback
              audioElement.pause();
              audioElement.currentTime = 0;
              stopAudioDataStream();
              
              notifyParent({ type: 'LOADING' });
              
              // Load new track
              audioElement.src = msg.url;
              audioElement.load();
              // Try to honor autoplay intent once metadata is available
              audioElement.addEventListener('canplay', attemptAutoplay, { once: true });
            }
            break;

          case 'PLAY':
            pendingAutoplay = false; // explicit play overrides
            if (vizPlayback) {
              vizPlayback.playing = true;
              postToVisualizers({ type: 'PLAYBACK_STATE', playing: true });
              notifyParent({ type: 'PLAYING' });
            }

            if (audioElement) {
              // When the bridge is actively playing audio, treat it as the source of truth.
              externalAudioFeed = false;
              log('[Bridge] Playing audio...');
              
              // Resume AudioContext if suspended
              if (audioContext && audioContext.state === 'suspended') {
                log('[Bridge] Resuming suspended AudioContext...');
                await audioContext.resume();
                log('[Bridge] AudioContext state:', audioContext.state);
              }
              
              try {
                await audioElement.play();
                log('[Bridge] Audio playing successfully');
                notifyParent({ type: 'PLAYING' });
              } catch (err) {
                console.error('[Bridge] Audio play failed:', err);
                notifyParent({ type: 'ERROR', error: err.message });
              }
            }
            break;

          case 'PAUSE':
            if (vizPlayback) {
              vizPlayback.playing = false;
              postToVisualizers({ type: 'PLAYBACK_STATE', playing: false });
              notifyParent({ type: 'PAUSED' });
            }

            if (audioElement) {
              audioElement.pause();
              notifyParent({ type: 'PAUSED' });
            }
            break;

          case 'STOP':
            if (vizPlayback) {
              stopVizPlayback();
            }
            if (audioElement) {
              audioElement.pause();
              audioElement.currentTime = 0;
              stopAudioDataStream();
              notifyParent({ type: 'PAUSED' });
            }
            break;

          case 'SEEK':
            if (vizPlayback && typeof msg.time === 'number') {
              const targetFrame = Math.max(0, Math.min(vizPlayback.frames - 1, Math.round(msg.time * Math.max(1, vizPlayback.fps || 30))));
              vizPlayback.frameIndex = targetFrame;
              notifyParent({
                type: 'TIME_UPDATE',
                currentTime: targetFrame / Math.max(1, vizPlayback.fps || 30),
                duration: vizPlayback.duration || (vizPlayback.frames / Math.max(1, vizPlayback.fps || 30))
              });
              break;
            }

            if (audioElement && typeof msg.time === 'number') {
              audioElement.currentTime = msg.time;
            }
            break;

          case 'SET_VOLUME':
            if (audioElement && typeof msg.volume === 'number') {
              audioElement.volume = Math.max(0, Math.min(1, msg.volume));
            }
            break;

          case 'SET_MUTED':
            if (audioElement) {
              const muted = !!msg.muted;
              // Keep element unmuted so analyser still sees signal; silence via gain.
              audioElement.muted = false;
              if (gainNode) gainNode.gain.value = muted ? 0 : 1;
            }
            break;

          case 'SET_RATE':
            if (audioElement && typeof msg.rate === 'number') {
              audioElement.playbackRate = msg.rate;
            }
            break;

          case 'AUDIO_DATA':
            // Live external feed wins over any internal playback/analyser.
            externalAudioFeed = true;
            // Stop any internal analyser loop and playback to avoid alternating frames.
            stopAudioDataStream();
            if (audioElement && !audioElement.paused) {
              try { audioElement.pause(); } catch { /* ignore */ }
            }
            if (vizPlayback) {
              stopVizPlayback({ silent: true });
            }
            try {
              // Forward audio data to both visualizer iframes
              postToVisualizers(msg);
            } catch (err) {
              if (LOG_VERBOSE) console.warn('[Bridge] Failed to forward AUDIO_DATA:', err);
            }
            break;

          case 'PING':
            notifyParent({ type: 'PONG', ready: visualizerReady });
            break;

          case 'LIST_VISUALIZER_MODULES':
            log('[Bridge] Received LIST_VISUALIZER_MODULES from parent');
            requestModuleList();
            break;

          case 'SET_VISUALIZER_MODULE':
            try {
              // Send module change to both iframes (they will sync via BroadcastChannel)
              postToVisualizers({ type: 'SET_MODULE', module: msg.module });
            } catch (err) {
              console.warn('[Bridge] Failed to switch module:', err);
            }
            break;
        }
      });

      // Send periodic time updates to parent
      setInterval(() => {
        if (vizPlayback) {
          const duration = vizPlayback.duration || (vizPlayback.frames / Math.max(1, vizPlayback.fps || 30));
          const currentTime = vizPlayback.frameIndex / Math.max(1, vizPlayback.fps || 30);
          notifyParent({ type: 'TIME_UPDATE', currentTime, duration });
          return;
        }

        if (!audioElement) return;
        
        notifyParent({
          type: 'TIME_UPDATE',
          currentTime: audioElement.currentTime,
          duration: audioElement.duration || 0
        });
      }, 250);

      // Detect BPM from audio and send to visualizer
      let bpmDetected = false;
      
      async function detectAndSendBPM() {
        if (bpmDetected || !audioContext || !analyserNode) {
          log('[Bridge] BPM detection skipped:', { bpmDetected, hasAudioContext: !!audioContext, hasAnalyser: !!analyserNode });
          return;
        }
        bpmDetected = true;
        
        log('[Bridge] üéµ Starting BPM detection from audio...');
        
        // Simple BPM detection using beat detection from frequency data
        const sampleRate = audioContext.sampleRate;
        const bufferSize = 2048;
        const data = new Uint8Array(analyserNode.frequencyBinCount);
        
        // Collect peaks over 10 seconds
        const peaks = [];
        const startTime = Date.now();
        const duration = 10000; // 10 seconds
        
        const detectPeaks = () => {
          analyserNode.getByteFrequencyData(data);
          
          // Calculate energy
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            sum += data[i];
          }
          const energy = sum / data.length;
          
          // Record timestamp if energy spike detected (lowered threshold)
          if (energy > 80) {
            peaks.push(Date.now());
          }
          
          if (Date.now() - startTime < duration) {
            setTimeout(detectPeaks, 50); // Sample every 50ms
          } else {
            // Calculate BPM from peaks
            log('[Bridge] BPM detection complete. Peaks found:', peaks.length);
            
            if (peaks.length > 2) {
              const intervals = [];
              for (let i = 1; i < peaks.length; i++) {
                intervals.push(peaks[i] - peaks[i-1]);
              }
              
              // Calculate average interval
              const avgInterval = intervals.reduce((a, b) => a + b) / intervals.length;
              const bpm = Math.round(60000 / avgInterval);
              
              log('[Bridge] ‚úì Detected BPM:', bpm, '(from', peaks.length, 'peaks, avg interval:', Math.round(avgInterval), 'ms)');
              
              // Send to both visualizer iframes
              postToVisualizers({
                type: 'BPM_DATA',
                bpm: bpm
              });
            } else {
              log('[Bridge] ‚ö†Ô∏è Not enough peaks for BPM detection (found', peaks.length, '), using default 120 BPM');
              postToVisualizers({
                type: 'BPM_DATA',
                bpm: 120
              });
            }
          }
        };
        
        detectPeaks();
      }
      
      // Send audio frequency data to visualizer for visualization
      function streamAudioDataToVisualizer() {
        if (!analyserNode || !visualizerReady) {
          audioDataRafId = null;
          return;
        }

        // If we have a live external audio feed (from the player), don't also stream
        // analyser frames from the bridge, otherwise visualizers may flicker between
        // real audio and silence.
        if (externalAudioFeed) {
          audioDataRafId = null;
          return;
        }

        // Only stream when the bridge's audio element is actively playing.
        if (!audioElement || audioElement.paused) {
          audioDataRafId = null;
          return;
        }
        
        const freqArray = new Uint8Array(analyserNode.frequencyBinCount);
        const timeArray = new Uint8Array(analyserNode.fftSize || analyserNode.frequencyBinCount);
        analyserNode.getByteFrequencyData(freqArray);
        analyserNode.getByteTimeDomainData(timeArray);

        // Compute a lightweight RMS for consumers that want loudness.
        let rms = 0;
        for (let i = 0; i < timeArray.length; i++) {
          const v = (timeArray[i] - 128) / 128; // -1..1
          rms += v * v;
        }
        rms = timeArray.length ? Math.sqrt(rms / timeArray.length) : 0;
        
        // Send to both visualizer iframes
        try {
          postToVisualizers({
            type: 'AUDIO_DATA',
            frequencyData: Array.from(freqArray), // Convert Uint8Array to regular array for postMessage
            timeData: Array.from(timeArray),
            bufferLength: analyserNode.frequencyBinCount,
            timeLength: timeArray.length,
            rms
          });
        } catch (err) {
          // Ignore errors
          if (LOG_VERBOSE && Math.random() < 0.01) {
            console.error('[Bridge] Error sending audio data:', err);
          }
        }

        audioDataRafId = requestAnimationFrame(streamAudioDataToVisualizer);
      }

      function stopAudioDataStream() {
        if (audioDataRafId != null) {
          cancelAnimationFrame(audioDataRafId);
          audioDataRafId = null;
        }
      }
      
      // Start streaming when audio plays
      function startAudioDataStream() {
        // If external audio feed is active, don't run internal analyser stream to avoid mixing zero frames.
        if (externalAudioFeed) return;
        if (audioDataRafId != null) return;
        if (audioElement && !audioElement.paused) {
          log('[Bridge] Starting audio data stream');
          audioDataRafId = requestAnimationFrame(streamAudioDataToVisualizer);
        }
      }

      // Load both visualizer iframes (dual-iframe architecture)
      function loadVisualizer() {
        let canvasReady = false;
        let guiReady = false;
        
        function checkBothReady() {
          if (canvasReady && guiReady) {
            loading.classList.add('hidden');
            visualizerReady = true;
            
            // Initialize both visualizer iframes
            initializeVisualizer();
            // Proactively fetch module list for robustness
            requestModuleList();
            if (modulePollTimer) clearInterval(modulePollTimer);
            modulePollTimer = setInterval(() => {
              if (moduleListReceived) {
                clearInterval(modulePollTimer);
                modulePollTimer = null;
                return;
              }
              if (Math.random() < 0.2) log('[Bridge] polling module list...');
              requestModuleList();
            }, 1000);
            
            notifyParent({ type: 'VISUALIZER_READY' });
          }
        }
        
        // Load canvas iframe (visualization only)
        canvasFrame.src = CANVAS_PATH;
        canvasFrame.addEventListener('load', () => {
          log('[Bridge] Canvas iframe loaded');
          canvasReady = true;
          checkBothReady();
        });
        canvasFrame.addEventListener('error', () => {
          console.warn('[Bridge] Failed to load canvas iframe from:', CANVAS_PATH);
          canvasReady = true; // Mark ready anyway to continue
          checkBothReady();
        });
        
        // Load GUI iframe (controls only)
        guiFrame.src = GUI_PATH;
        guiFrame.addEventListener('load', () => {
          log('[Bridge] GUI iframe loaded');
          guiReady = true;
          checkBothReady();
        });
        guiFrame.addEventListener('error', () => {
          console.warn('[Bridge] Failed to load GUI iframe from:', GUI_PATH);
          guiReady = true; // Mark ready anyway to continue
          checkBothReady();
        });
        
        // Timeout fallback
        setTimeout(() => {
          if (!visualizerReady) {
            loading.textContent = 'Visualizer loading slowly or unavailable';
            visualizerReady = true;
            notifyParent({ type: 'VISUALIZER_READY' });
          }
        }, 5000);
      }

      // Auto-initialize
      if (window.self !== window.top) {
        // We're in an iframe (called from Polaris)
        loadVisualizer();
        
        // Initialize audio after a short delay
        setTimeout(() => {
          initAudio();
        }, 500);
      } else {
        // Standalone mode - just show the visualizer
        loading.textContent = 'Standalone mode - load this in Polaris player';
        loadVisualizer();
      }
    </script>
  </body>
</html>
